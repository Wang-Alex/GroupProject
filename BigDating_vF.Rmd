---
title: "TEAM BIG DATING ANALYSIS"
author: "Alex Wang, Peter-Paul de Leeuw, Paulo Nakamura, Hicham Aber, Philipp Hein"
date: "31 January 2017"
output: html_document
---

<center>![BIG DATING ANALLYSIS](http://immigrantsdating.com/wp-content/uploads/2014/12/couple-dating.jpg)</center>




```{r echo=FALSE, eval=FALSE, tidy=TRUE}
##setwd("/GroupProject") COMMENT:We don't need to set a new directory because the project opens in the right directory anyway
list.files()
```

```{r echo=FALSE, eval=TRUE, tidy=TRUE}
d <- read.csv("./Data/Testdata.csv", sep=",", dec=".")
d=data.matrix(d)
```
# Introduction

As often happens in groups of 5 males, the conversation about what to analyze for the Big Data & Analytics course quickly driften towards girls as the main topic. Instead of feeling quilty about that, we decided to combine the two and help the 4 singles in our group to understand their chances on the singles market better.  The dataset we will use for this exercise is a set of speed dating data gathered by Raymond Fisman, et al. (2006) at Columbia University. The data is made up by survey evaluations of `r nrow(d)` speed dating events, for which `r ncol(d)` variables are collected. The data includes the following characteristics of these speed dates:
<br>- Detailed profile of each speed dating participant, including wages, field of study, race and income
<br>- characteristics most important to them in a future partner 
<br>- scores of the persons they speed dated and whether there were regular follow ups after the speeddate
<br>
<br>We decided to use the data for two purposes to understand what kind of people join speeddating events, and thus whether it is worthwhile to attend such an event. We will thus analyze the characteristics of people attending and create a number of generic profiles of typical people attending.
<br>

###Process Description

<br>
The initial stage of our project was to set up a proper process before actually analyzing the dataset with R. We came up with the following approach:
<br>1. Review the information in the csv-dataset
<br>2. Define the business problem and specify which exact analyses are required
<br>3. Transform the dataset, in order to make it more useful for the purpose of the exercise (e.g., transformed non-numerical entries in converted them into numerical values)
<br>4. Conduct the analyses
<br>5. Perform quality checks for the analyses
<br>6. Qualitatively answer the business questions at hand
<br>

###Description of the data

<br>
The data we worked with is from a public source (https://www.kaggle.com/annavictoria/speed-dating-experiment). The original source is from an experiment conducted at Columbia Business School. The initial purpose of the analysis was to better understand the âGender Differences in Mate Selection: Evidence From a Speed Dating Experimentâ. 
<br>The data is in a table format. Columns comprise different characteristics of the participants (e.g., gender, income, background, race). In the initial dataset each row comprise a date with another person. However, for simplification reasons, we manually adjusted the data so that each line only represents a person (characterized by a unique ID). Hence, our analysis only assesses information about the participants â weâre less concerned about the quality or outcome of the dates.
<br>

###Business Purpose

<br>
Since the exercise requires us to apply data within a business context, weâll focus our assessment on 2 main purposes. We take the standpoint of a speed dating agency, which â based on historical data â wants to better understand its clientele.
<br>First of all, our agency wants to create an advertising campaign to attract more people to join our events. In order to make statements about our client base, we initially need to use descriptive analysis to retrieve basic information about our current customers. We are specifically interested in the following information: 
<br>Gender distribution: Many speed dating agencies are struggling to keep a fair share of women and men in their database. If we can prove that we have an equal share of genders, this makes our agency significantly more interesting
<br>Occupation: It would be great if we could prove that we have a certain number of members in all different professions/ income brackets. This will help us to position our agency towards a certain clientele (e.g., as done by âelitepartner.deâ in Germany).
<br>Purpose of the speed dating: We need to better understand, what we can claim towards our potential members. Are we an agency focusing on one-night-stands or do we seriously claim to create couples for life?
<br>Race: Since people have preferences towards dating people with certain races (e.g., Native American, Asian), we need to assess which races our current membership is composed of.
<br>

###Descriptive statistics

<br>
The table below shows data from the first 50 respondents:
<br>
<br>

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
show_data = data.frame(round(d,2))[1:50,]
show_data$Variables = rownames(show_data)
library(googleVis)
m1<-gvisTable(show_data,options=list(showRowNumber=TRUE,width=1220, height=min(400,27*(nrow(show_data)+1)),allowHTML=TRUE,page='disable'))
print(m1,'chart')
```
<br>

The following pie chart shows us the gender distribution:
<br>

````{r echo=FALSE, eval=TRUE, tidy=TRUE}
d <- read.csv("./Data/Testdata.csv", sep=",", dec=".")
d=data.matrix(d)

male <- length(which(d[,"gender"] == 1))
female <- length(which(d[,"gender"] == 0))


slices <- c(female, male)
lbls <- c("Female", "Male")
pie(slices, labels = lbls, main="Distribution of Gender")
```


We have displayed the data


The bar chart below shows the educational backgrounds:
<br>
````{r echo=FALSE, eval=TRUE, tidy=TRUE}
Law <- length(which(d[,"field_cd"] == 1))
Math <- length(which(d[,"field_cd"] == 2))
SocialScience <- length(which(d[,"field_cd"] == 3))
MedScience <- length(which(d[,"field_cd"] == 4))
Engineering <- length(which(d[,"field_cd"] == 5))
Journalism <- length(which(d[,"field_cd"] == 6))
History <- length(which(d[,"field_cd"] == 7))
Business <- length(which(d[,"field_cd"] == 8))
Education <- length(which(d[,"field_cd"] == 9))
BiologicalSciences <- length(which(d[,"field_cd"] == 10))
SocialWork  <- length(which(d[,"field_cd"] == 11))
Undergrad  <- length(which(d[,"field_cd"] == 12))
PoliticalScience <- length(which(d[,"field_cd"] == 13))
Film <- length(which(d[,"field_cd"] == 14))
Arts <- length(which(d[,"field_cd"] == 15))
Languages <- length(which(d[,"field_cd"] == 16))
Architecture <- length(which(d[,"field_cd"] == 17))
Other <- length(which(d[,"field_cd"] == 18))

countfield <- c(Law, Math, SocialScience, MedScience, Engineering, Journalism, History, Business, Education, BiologicalSciences, SocialWork, Undergrad, PoliticalScience, Film, Arts, Languages, Architecture, Other)

barplot(countfield, main="Field of Study", names.arg=c("Law", "Math", "SocialSc.", "MedSc.", "Enginrng", "Journalism", "History", "Business", "Education", "Biol. Sc.", "SocialWrk", "Undergrad", "Pol. Sc.", "Film", "Arts", "Languages", "Architect.", "Other"), las=2)
```
<br>First, we displayed the data by field of study: As the experimentation was run with MBA's students most of the candidates have a business background, the next most representative's population are from biology and engineering background. The last group are from law, social science and political science. These 3 groups represent more than 80% of our sample.

<br>
The following bar chart shows us the distribution on why the people wanted to go on a date:
<br>

````{r echo=FALSE, eval=TRUE, tidy=TRUE}
Seemedlikeafunnightout <- length(which(d[,"goal"] == 1))
Tomeetnewpeople <- length(which(d[,"goal"] == 2))
Togetadate <- length(which(d[,"goal"] == 3))
Lookingforaseriousrelationship <- length(which(d[,"goal"] == 4))
TosayIdidit <- length(which(d[,"goal"] == 5))
Other <- length(which(d[,"goal"] == 6))

countgoals <- c(Seemedlikeafunnightout, Tomeetnewpeople, Togetadate, Lookingforaseriousrelationship, TosayIdidit, Other)

barplot(countgoals, main="Goals for Speed Dating", names.arg=c("Fun night", "Meet peopl", "Get a Date", "Relationsh.", "Say I did it", "Other"), las=2)
````

<br>Second, we wanted to know what is the goal that the candidates are looking for when coming to speed dating: Surprisingly most of the candidates were looking for a one night or just meet other people.
 

The following histogram shows the self reported race:
<br>
```{r echo=FALSE, eval=TRUE, tidy=TRUE}

Black <- length(which(d[,"race"] == 1))
Caucasian <- length(which(d[,"race"] == 2))
Latino <- length(which(d[,"race"] == 3))
Asian <- length(which(d[,"race"] == 4))
NativeAm <- length(which(d[,"race"] == 5))
Other <- length(which(d[,"race"] == 6))

countrace <- c(Black, Caucasian, Latino, Asian, NativeAm, Other)

barplot(countgoals, main="Race", names.arg=c("Black", "Caucasian", "Latino", "Asian", "Native Am", "Other"), las=2)

```
Third we have segmented the candidates by race to see if there is any ethnicity influence in our results: The biggest populations are black and Caucasian. 

###Grouping typical speed daters together

```{r setuplibraries, echo=FALSE, message=FALSE}
suppressWarnings(source("./AnalyticsLibraries/library.R"))
# Package options
suppressWarnings(ggthemr('fresh'))  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.35, results="asis")
options(knitr.kable.NA = '')
```


# The Data

First we load the data to use (see the raw .Rmd file to change the data file as needed):

```{r setupdata1E, echo=TRUE, tidy=TRUE}
# Please ENTER the name of the file with the data used. The file should be a .csv with one row per observation (e.g. person) and one column per attribute. Do not add .csv at the end, make sure the data are numeric.
datafile_name = "./Data/Testdata.csv"

# Please enter the minimum number below which you would like not to print - this makes the readability of the tables easier. Default values are either 10e6 (to print everything) or 0.5. Try both to see the difference.
MIN_VALUE = 0.5

# Please enter the maximum number of observations to show in the report and slides. 
# DEFAULT is 10. If the number is large the report may be slow.
max_data_report = 10
```

```{r}
d <- read.csv(datafile_name)
d <- data.matrix(d) 
ProjectData_INITIAL <- d

```


# Part 1: Key Customer Characteristics


```{r setupfactor, echo=TRUE, tidy=TRUE}
# Please ENTER then original raw attributes to use. 
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
factor_attributes_used = c(8:17)

# Please ENTER the selection criterions for the factors to use. 
# Choices: "eigenvalue", "variance", "manual"
factor_selectionciterion = "manual"

# Please ENTER the desired minumum variance explained 
# (Only used in case "variance" is the factor selection criterion used). 
minimum_variance_explained = 65  # between 1 and 100

# Please ENTER the number of factors to use 
# (Only used in case "manual" is the factor selection criterion used).
manual_numb_factors_used = 7

# Please ENTER the rotation eventually used (e.g. "none", "varimax", "quatimax", "promax", "oblimin", "simplimax", and "cluster" - see help(principal)). Default is "varimax"
rotation_used = "varimax"

```

```{r}
factor_attributes_used <- intersect(factor_attributes_used, 1:ncol(d))
ProjectDataFactor <- d[,factor_attributes_used]
ProjectDataFactor <- d <- data.matrix(ProjectDataFactor)
```

## Steps 1-2: Check the Data 


Start by some basic visual exploration of, say, a few data:

```{r}
rownames(ProjectDataFactor) <- paste0("Date ", sprintf("%02i", 1:nrow(ProjectDataFactor)))
iprint.df(t(head(round(ProjectDataFactor, 2), max_data_report)))
```

The data we use here have the following descriptive statistics: 

```{r}
iprint.df(round(my_summary(ProjectDataFactor), 2))
```



## Step 3: Check Correlations

This is the correlation matrix of the customer responses to the `r ncol(ProjectDataFactor)` attitude questions - which are the only questions that we will use for the segmentation (see the case):

```{r}
thecor = round(cor(ProjectDataFactor),2)
iprint.df(round(thecor,2), scale=TRUE)
```


## Step 4: Choose number of factors


```{r}
# Here is how the `principal` function is used 
UnRotated_Results<-principal(ProjectDataFactor, nfactors=ncol(ProjectDataFactor), rotate="none",score=TRUE)
UnRotated_Factors<-round(UnRotated_Results$loadings,2)
UnRotated_Factors<-as.data.frame(unclass(UnRotated_Factors))
colnames(UnRotated_Factors)<-paste("Comp",1:ncol(UnRotated_Factors),sep="")
```

```{r}
# Here is how we use the `PCA` function 
Variance_Explained_Table_results<-PCA(ProjectDataFactor, graph=FALSE)
Variance_Explained_Table<-Variance_Explained_Table_results$eig
Variance_Explained_Table_copy<-Variance_Explained_Table

rownames(Variance_Explained_Table) <- paste("Component", 1:nrow(Variance_Explained_Table), sep=" ")
colnames(Variance_Explained_Table) <- c("Eigenvalue", "Pct of explained variance", "Cumulative pct of explained variance")
```

Let's look at the **variance explained** as well as the **eigenvalues** (see session readings):

```{r}
iprint.df(round(Variance_Explained_Table, 2))
```

```{r}
eigenvalues  <- Variance_Explained_Table[, "Eigenvalue"]
df           <- cbind(as.data.frame(eigenvalues), c(1:length(eigenvalues)), rep(1, length(eigenvalues)))
colnames(df) <- c("eigenvalues", "components", "abline")
iplot.df(melt(df, id="components"))
```


## Step 5: Interpret the factors

Let's now see how the "top factors" look like. 

```{r}
if (factor_selectionciterion == "eigenvalue")
  factors_selected = sum(Variance_Explained_Table_copy[,1] >= 1)
if (factor_selectionciterion == "variance")
  factors_selected = 1:head(which(Variance_Explained_Table_copy[,"cumulative percentage of variance"]>= minimum_variance_explained),1)
if (factor_selectionciterion == "manual")
  factors_selected = manual_numb_factors_used
```


To better visualize them, we will use what is called a "rotation". There are many rotations methods. In this case we selected the `r rotation_used` rotation. For our data, the `r factors_selected` selected factors look as follows after this rotation: 

```{r}
Rotated_Results<-principal(ProjectDataFactor, nfactors=max(factors_selected), rotate=rotation_used,score=TRUE)
Rotated_Factors<-round(Rotated_Results$loadings,2)
Rotated_Factors<-as.data.frame(unclass(Rotated_Factors))
colnames(Rotated_Factors)<-paste("Comp.",1:ncol(Rotated_Factors),sep="")

sorted_rows <- sort(Rotated_Factors[,1], decreasing = TRUE, index.return = TRUE)$ix
Rotated_Factors <- Rotated_Factors[sorted_rows,]

iprint.df(Rotated_Factors, scale=TRUE)
```

To better visualize and interpret the factors we often "suppress" loadings with small values, e.g. with absolute values smaller than 0.5. In this case our factors look as follows after suppressing the small numbers:

```{r}
Rotated_Factors_thres <- Rotated_Factors
Rotated_Factors_thres[abs(Rotated_Factors_thres) < MIN_VALUE]<-NA
colnames(Rotated_Factors_thres)<- colnames(Rotated_Factors)
rownames(Rotated_Factors_thres)<- rownames(Rotated_Factors)

iprint.df(Rotated_Factors_thres, scale=TRUE)
```

## Step 6:  Save factor scores 

We can now either replace all initial variables used in this part with the factors scores or just select one of the initial variables for each of the selected factors in order to represent that factor. Here is how the factor scores  are for the first few respondents:

```{r}
NEW_ProjectData <- round(Rotated_Results$scores[,1:factors_selected,drop=F],2)
colnames(NEW_ProjectData)<-paste("DV (Factor)",1:ncol(NEW_ProjectData),sep=" ")

iprint.df(t(head(NEW_ProjectData, 10)), scale=TRUE)
```


# Part 2: Dating Segmentation 

```{r setupcluster, echo=TRUE, tidy=TRUE}
# Please ENTER then original raw attributes to use for the segmentation (the "segmentation attributes")
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
segmentation_attributes_used = c(14,12,13,11) #c(10,19,5,12,3) 

# Please ENTER then original raw attributes to use for the profiling of the segments (the "profiling attributes")
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
profile_attributes_used = c(2:17) 

# Please ENTER the number of clusters to eventually use for this report
numb_clusters_used = 3 # for  possibly use 5, for Mall_Visits use 3

# Please enter the method to use for the segmentation:
profile_with = "hclust" #  "hclust" or "kmeans"

# Please ENTER the distance metric eventually used for the clustering in case of hierarchical clustering 
# (e.g. "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski" - see help(dist)). 
# DEFAULT is "euclidean"
distance_used = "euclidean"

# Please ENTER the hierarchical clustering method to use (options are:
# "ward", "single", "complete", "average", "mcquitty", "median" or "centroid").
# DEFAULT is "ward"
hclust_method = "ward.D"

# Please ENTER the kmeans clustering method to use (options are:
# "Hartigan-Wong", "Lloyd", "Forgy", "MacQueen").
# DEFAULT is "Lloyd"
kmeans_method = "Lloyd"

```


```{r}
# Same as the initial data
ProjectData <- ProjectData_INITIAL

segmentation_attributes_used <- intersect(segmentation_attributes_used, 1:ncol(ProjectData))
profile_attributes_used <- intersect(profile_attributes_used, 1:ncol(ProjectData))

ProjectData_segment <- ProjectData[,segmentation_attributes_used]
ProjectData_profile <- ProjectData[,profile_attributes_used]

ProjectData_scaled <- apply(ProjectData, 2, function(r) if (sd(r)!=0) (r-mean(r))/sd(r) else 0*r)
```


```{r}
euclidean_pairwise <- as.matrix(dist(head(ProjectData_segment, max_data_report), method="euclidean"))
euclidean_pairwise <- euclidean_pairwise*lower.tri(euclidean_pairwise) + euclidean_pairwise*diag(euclidean_pairwise) + 10e10*upper.tri(euclidean_pairwise)
euclidean_pairwise[euclidean_pairwise==10e10] <- NA
rownames(euclidean_pairwise) <- colnames(euclidean_pairwise) <- sprintf("Date.%02d", 1:max_data_report)

iprint.df(round(euclidean_pairwise))
```

## Step 5: Visualize Pair-wise Distances

We can see the histogram of, say, the first 2 variables (can you change the code chunk in the raw .Rmd file to see other variables?)

```{r}
variables_to_plot = 1:2
do.call(iplot.grid, lapply(variables_to_plot, function(n){
  iplot.hist(ProjectData_segment[, n], breaks=5, xlab = paste("Variable", n))
}))
```

```{r}
Pairwise_Distances <- dist(ProjectData_segment, method = distance_used) 
iplot.hist(Pairwise_Distances, breaks=10)
```

## Step 6: Method and Number of Segments

We need to select the clustering method to use, as well as the number of cluster. It may be useful to see the dendrogram from Hierarchical Clustering, to have a quick idea of how the data may be segmented and how many segments there may be. Here is the dendrogram for our data:

```{r}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)
# Display dendogram
iplot.dendrogram(Hierarchical_Cluster)
# TODO: Draw dendogram with red borders around the 3 clusters
# rect.hclust(Hierarchical_Cluster, k=numb_clusters_used, border="red") 
```

We can also plot the "distances" traveled before we need to merge any of the lower and smaller in size clusters into larger ones - the heights of the tree branches that link the clusters as we traverse the tree from its leaves to its root. If we have n observations, this plot has n-1 numbers, we see the first 20 here. 
```{r}
num <- nrow(ProjectData) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
iplot.df(melt(head(df1, 20), id="index"), xlab="Number of Components")
```

Here is the segment membership of the first `r max_data_report` respondents if we use hierarchical clustering:

```{r}
cluster_memberships_hclust <- as.vector(cutree(Hierarchical_Cluster, k=numb_clusters_used)) # cut tree into 3 clusters
cluster_ids_hclust=unique(cluster_memberships_hclust)

ProjectData_with_hclust_membership <- cbind(1:length(cluster_memberships_hclust),cluster_memberships_hclust)
colnames(ProjectData_with_hclust_membership)<-c("Observation Number","Cluster_Membership")

iprint.df(round(head(ProjectData_with_hclust_membership, max_data_report), 2))
```

while this is the segment membership if we use k-means:

```{r}
kmeans_clusters <- kmeans(ProjectData_segment,centers= numb_clusters_used, iter.max=2000, algorithm=kmeans_method)

ProjectData_with_kmeans_membership <- cbind(1:length(kmeans_clusters$cluster),kmeans_clusters$cluster)
colnames(ProjectData_with_kmeans_membership)<-c("Observation Number","Cluster_Membership")

iprint.df(round(head(ProjectData_with_kmeans_membership, max_data_report), 2))
```

## Step 7: Profile and interpret the segments 

In market segmentation one may use variables to **profile** the segments which are not the same (necessarily) as those used to **segment** the market: the latter may be, for example, attitude/needs related (you define segments based on what the customers "need"), while the former may be any information that allows a company to identify the defined customer segments (e.g. demographics, location, etc). Of course deciding which variables to use for segmentation and which to use for profiling (and then **activation** of the segmentation for business purposes) is largely subjective.  In this case we can use all survey questions for profiling for now - the `profile_attributes_used` variables selected below. 

There are many ways to do the profiling of the segments. For example, here we show how the *average* answers of the respondents *in each segment* compare to the *average answer of all respondents* using the ratio of the two.  The idea is that if in a segment the average response to a question is very different (e.g. away from ratio of 1) than the overall average, then that question may indicate something about the segment relative to the total population. 

Here are for example the profiles of the segments using the clusters found above.  First let's see just the average answer people gave to each question for the different segments as well as the total population:

```{r}
cluster_memberships_kmeans <- kmeans_clusters$cluster 
cluster_ids_kmeans <- unique(cluster_memberships_kmeans)

if (profile_with == "hclust"){
  cluster_memberships <- cluster_memberships_hclust
  cluster_ids <-  cluster_ids_hclust  
}
if (profile_with == "kmeans"){
  cluster_memberships <- cluster_memberships_kmeans
  cluster_ids <-  cluster_ids_kmeans
}

# WE WILL USE THESE IN THE CLASSIFICATION PART LATER
NewData = matrix(cluster_memberships,ncol=1)

population_average = matrix(apply(ProjectData_profile, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_profile[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData_profile) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Seg.", 1:length(cluster_ids), sep="")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)

iprint.df(round(cluster.profile, 2))
```

We can also "visualize" the segments using **snake plots** for each cluster. For example, we can plot the means of the profiling variables for each of our clusters to better visualize differences between segments. For better visualization we plot the standardized profiling variables.

```{r}
ProjectData_scaled_profile = ProjectData_scaled[, profile_attributes_used,drop=F]

Cluster_Profile_standar_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_scaled_profile[(cluster_memberships==i), ,drop = F], 2, mean))
if (ncol(ProjectData_scaled_profile) < 2)
  Cluster_Profile_standar_mean = t(Cluster_Profile_standar_mean)
colnames(Cluster_Profile_standar_mean) <- paste("Seg ", 1:length(cluster_ids), sep="")

iplot.df(melt(cbind.data.frame(idx=as.numeric(1:nrow(Cluster_Profile_standar_mean)), Cluster_Profile_standar_mean), id="idx"), xlab="Profiling variables (standardized)",  ylab="Mean of cluster")
```

We can also compare the averages of the profiling variables of each segment relative to the average of the variables across the whole population. This can also help us better understand whether  there are indeed clusters in our data (e.g. if all segments are much like the overall population, there may be no segments). For example, we can measure the ratios of the average for each cluster to the average of the population, minus 1, (e.g. `avg(cluster)` `/` `avg(population)` `-1`) for each segment and variable:

```{r}
population_average_matrix <- population_average[,"Population",drop=F] %*% matrix(rep(1,ncol(Cluster_Profile_mean)),nrow=1)
cluster_profile_ratios <- (ifelse(population_average_matrix==0, 0,Cluster_Profile_mean/population_average_matrix))
colnames(cluster_profile_ratios) <- paste("Seg.", 1:ncol(cluster_profile_ratios), sep="")
rownames(cluster_profile_ratios) <- colnames(ProjectData)[profile_attributes_used]
## printing the result in a clean-slate table
iprint.df(round(cluster_profile_ratios-1, 2))
```



